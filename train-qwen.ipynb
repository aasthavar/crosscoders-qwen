{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/crosscoders_p312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning:\n",
      "\n",
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2.5-0.5B into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2.5-0.5B-Instruct into HookedTransformer\n",
      "Data is not cached. Loading data from HF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 295/295 [00:00<00:00, 2.67kB/s]\n",
      "Downloading data: 100%|██████████| 100M/100M [00:01<00:00, 57.5MB/s] \n",
      "Downloading data: 100%|██████████| 99.7M/99.7M [00:01<00:00, 60.7MB/s]\n",
      "Downloading data: 100%|██████████| 101M/101M [00:01<00:00, 60.5MB/s] \n",
      "Downloading data: 100%|██████████| 99.8M/99.8M [00:01<00:00, 60.1MB/s]\n",
      "Downloading data: 100%|██████████| 101M/101M [00:04<00:00, 24.6MB/s] \n",
      "Downloading data: 100%|██████████| 99.7M/99.7M [00:02<00:00, 33.6MB/s]\n",
      "Downloading data: 100%|██████████| 101M/101M [00:01<00:00, 57.2MB/s] \n",
      "Downloading data: 100%|██████████| 101M/101M [00:01<00:00, 64.0MB/s] \n",
      "Downloading data: 100%|██████████| 99.8M/99.8M [00:02<00:00, 41.4MB/s]\n",
      "Generating train split: 100%|██████████| 536465/536465 [00:08<00:00, 63161.40 examples/s]\n",
      "Saving the dataset (9/9 shards): 100%|██████████| 536465/536465 [00:44<00:00, 12095.47 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokens to disk\n",
      "n_layers: 24\n",
      "d_model: 896\n",
      "n_heads: 14\n",
      "d_head: 64\n",
      "d_mlp: 4864\n",
      "d_vocab: 151936\n",
      "model_name: qwen2.5-0.5b\n"
     ]
    }
   ],
   "source": [
    "# choose environment = crosscoders_p312\n",
    "\n",
    "from utils import *\n",
    "import torch\n",
    "from trainer import Trainer\n",
    "\n",
    "device = 'cuda:0'\n",
    "# torch.set_grad_enabled(False) # important for memory\n",
    "base_model_id = \"Qwen/Qwen2.5-0.5B\" # \"Qwen/Qwen2.5-1.5B\",\n",
    "chat_model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"  # \"Qwen/Qwen2.5-1.5B-Instruct\", \n",
    "# local_dataset_path = \"data/pile-lmsys-mix-300k-tokenized-qwen2x5\"\n",
    "# hf_dataset_path = \"aastha6/pile-lmsys-mix-300k-tokenized-qwen2.5\"\n",
    "local_dataset_path = \"data/pile-lmsys-mix-500k-tokenized-qwen2x5\"\n",
    "hf_dataset_path = \"aastha6/pile-lmsys-mix-500k-tokenized-qwen2.5\"\n",
    "\n",
    "\n",
    "base_model = HookedTransformer.from_pretrained(\n",
    "    base_model_id,\n",
    "    device=device, \n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "chat_model = HookedTransformer.from_pretrained(\n",
    "    chat_model_id,\n",
    "    device=device,\n",
    "    dtype=torch.bfloat16 \n",
    ")\n",
    "\n",
    "all_tokens = load_pile_lmsys_mixed_tokens(\n",
    "    local_dataset_path=local_dataset_path,\n",
    "    hf_dataset_path=hf_dataset_path,\n",
    ")\n",
    "\n",
    "n_layers = base_model.cfg.n_layers\n",
    "d_model = base_model.cfg.d_model\n",
    "n_heads = base_model.cfg.n_heads\n",
    "d_head = base_model.cfg.d_head\n",
    "d_mlp = base_model.cfg.d_mlp\n",
    "d_vocab = base_model.cfg.d_vocab\n",
    "\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"d_model: {d_model}\")\n",
    "print(f\"n_heads: {n_heads}\")\n",
    "print(f\"d_head: {d_head}\")\n",
    "print(f\"d_mlp: {d_mlp}\")\n",
    "print(f\"d_vocab: {d_vocab}\")\n",
    "\n",
    "model_name = base_model_id.split(\"/\")[-1].lower()\n",
    "print(f\"model_name: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In IPython - skipped argparse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating norm scaling factor: 100%|██████████| 100/100 [00:12<00:00,  8.15it/s]\n",
      "Estimating norm scaling factor: 100%|██████████| 100/100 [00:12<00:00,  8.33it/s]\n",
      "100%|██████████| 128/128 [00:30<00:00,  4.14it/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvarmaaastha6\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/crosscoders-r1-distill-qwen/wandb/run-20250227_205304-fzzw837c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/varmaaastha6/crosscoders-r1-distill-qwen/runs/fzzw837c' target=\"_blank\">still-shape-24</a></strong> to <a href='https://wandb.ai/varmaaastha6/crosscoders-r1-distill-qwen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/varmaaastha6/crosscoders-r1-distill-qwen' target=\"_blank\">https://wandb.ai/varmaaastha6/crosscoders-r1-distill-qwen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/varmaaastha6/crosscoders-r1-distill-qwen/runs/fzzw837c' target=\"_blank\">https://wandb.ai/varmaaastha6/crosscoders-r1-distill-qwen/runs/fzzw837c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "default_cfg = {\n",
    "    \"seed\": 49,\n",
    "    \"batch_size\": 4096, # 128\n",
    "    \"buffer_mult\": 128,\n",
    "    \"lr\": 5e-5,\n",
    "    # \"num_tokens\": 200_000_000,\n",
    "    \"num_tokens\": 500_000_000,\n",
    "    \"l1_coeff\": 2,\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.999,\n",
    "    \"d_in\": base_model.cfg.d_model,\n",
    "    \"dict_size\": 2**14,\n",
    "    \"seq_len\": 1024,\n",
    "    \"enc_dtype\": \"fp32\",\n",
    "    \"model_name\": model_name,\n",
    "    \"site\": \"resid_pre\",\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"model_batch_size\": 4, # 1\n",
    "    \"log_every\": 100,\n",
    "    \"save_every\": 30000,\n",
    "    \"dec_init_norm\": 0.08,\n",
    "    \"hook_point\": \"blocks.12.hook_resid_pre\",\n",
    "    \"wandb_project\": \"crosscoders-r1-distill-qwen\",\n",
    "    \"wandb_entity\": \"varmaaastha6\",\n",
    "}\n",
    "cfg = arg_parse_update_cfg(default_cfg)\n",
    "\n",
    "trainer = Trainer(cfg, base_model, chat_model, all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:05<00:00, 12.64it/s]3it/s]\n",
      "100%|██████████| 64/64 [00:05<00:00, 12.64it/s]37it/s]\n",
      "100%|██████████| 64/64 [00:05<00:00, 12.60it/s]38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.65it/s]38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.64it/s]36it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.59it/s]38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.62it/s]37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.59it/s]36it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.58it/s]38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.61it/s]38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.59it/s]37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.59it/s]38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.61it/s]37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.60it/s]37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.64it/s]37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.63it/s].37it/s]\n",
      "100%|██████████| 64/64 [00:05<00:00, 12.63it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.65it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.72it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.66it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.65it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.68it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.67it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.67it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.66it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.60it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.60it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.75it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.78it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.79it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.77it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.74it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.64it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.65it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.65it/s].35it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.65it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.65it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.66it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.69it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.69it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.70it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.69it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.73it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.76it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.76it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.73it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.78it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.75it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.68it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.67it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.64it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.64it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.68it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.68it/s].36it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.68it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.62it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.59it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.64it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.59it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.62it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.62it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.60it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.64it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.69it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.68it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.69it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.72it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.71it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.58it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.60it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.52it/s].37it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.71it/s].38it/s] \n",
      "100%|██████████| 64/64 [00:05<00:00, 12.61it/s].36it/s] \n",
      "  4%|▍         | 4639/122070 [11:57<2:27:30, 13.27it/s] "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 122070"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run data is saved locally in /home/ubuntu/crosscoders-r1-distill-qwen/wandb/run-20250227_153033-zihcmacm\n",
    "# Syncing run rich-serenity-23 to Weights & Biases (docs)\n",
    "# View project at https://wandb.ai/varmaaastha6/crosscoders-r1-distill-qwen\n",
    "# View run at https://wandb.ai/varmaaastha6/crosscoders-r1-distill-qwen/runs/zihcmacm\n",
    "\n",
    "\n",
    "# Saved as version 1 in /home/ubuntu/crosscoders-r1-distill-qwen/checkpoints/version_2\n",
    "# final: saved model\n",
    "# CPU times: user 2h 9min 29s, sys: 15.4 s, total: 2h 9min 44s\n",
    "# Wall time: 2h 6min 53s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import huggingface_hub\n",
    "\n",
    "def push_to_hub(local_dir):\n",
    "    if isinstance(local_dir, huggingface_hub.Repository):\n",
    "        local_dir = local_dir.local_dir\n",
    "    os.system(f\"git -C {local_dir} add .\")\n",
    "    os.system(f\"git -C {local_dir} commit -m 'Auto Commit'\")\n",
    "    os.system(f\"git -C {local_dir} push origin main --force\")\n",
    "\n",
    "\n",
    "def upload_folder_to_hf(folder_path, repo_name=None, debug=False):\n",
    "    \"\"\"\n",
    "    Uploads a folder to HuggingFace, and creates a repo for it.\n",
    "    \"\"\"\n",
    "    folder_path = Path(folder_path)\n",
    "    if repo_name is None:\n",
    "        repo_name = folder_path.name\n",
    "    repo_folder = folder_path.parent / (folder_path.name + \"_repo\")\n",
    "    print(f\"repo_folder: {repo_folder}\")\n",
    "    print(f\"folder_path: {folder_path}\")\n",
    "    repo_url = huggingface_hub.create_repo(repo_name, exist_ok=True)\n",
    "    repo = huggingface_hub.Repository(repo_folder, repo_url)\n",
    "\n",
    "    for file in folder_path.iterdir():\n",
    "        if debug:\n",
    "            print(file.name)\n",
    "        file.rename(repo_folder / file.name)\n",
    "    push_to_hub(repo.local_dir)\n",
    "   \n",
    "model_name = \"qwen2.5-0.5b\"\n",
    "upload_folder_to_hf(folder_path=f\"checkpoints/version_2\", repo_name=f\"crosscoders-{model_name}\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 300k dataste\n",
    "\n",
    "# /home/ubuntu/miniconda3/envs/crosscoders_p312/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
    "# For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
    "#   warnings.warn(warning_message, FutureWarning)\n",
    "# Cloning https://huggingface.co/aastha6/crosscoders-qwen2.5-0.5b into local empty directory.\n",
    "# WARNING:huggingface_hub.repository:Cloning https://huggingface.co/aastha6/crosscoders-qwen2.5-0.5b into local empty directory.\n",
    "# repo_folder: checkpoints/version_2_repo\n",
    "# folder_path: checkpoints/version_2\n",
    "# Moving checkpoints/version_2/0.pt to checkpoints/version_2_repo/0.pt\n",
    "# Moving checkpoints/version_2/0_cfg.json to checkpoints/version_2_repo/0_cfg.json\n",
    "# Moving checkpoints/version_2/1.pt to checkpoints/version_2_repo/1.pt\n",
    "# Moving checkpoints/version_2/1_cfg.json to checkpoints/version_2_repo/1_cfg.json\n",
    "# [main 4f81f30] Auto Commit\n",
    "#  4 files changed, 8 insertions(+)\n",
    "#  create mode 100644 0.pt\n",
    "#  create mode 100644 0_cfg.json\n",
    "#  create mode 100644 1.pt\n",
    "#  create mode 100644 1_cfg.json\n",
    "# Uploading LFS objects: 100% (2/2), 480 MB | 56 MB/s, done.\n",
    "# To https://huggingface.co/aastha6/crosscoders-qwen2.5-0.5b\n",
    "#    593aea3..4f81f30  main -> main"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crosscoders_p312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
